{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNgnwrwRjEf553aB2FIHWlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salta-ak/Google-trend-forecast-with-LSTM/blob/main/experiment1_table2/RNN/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiP1rQmDw6Je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qmEKJ-N5F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757ea8c8-7845-4c5d-8e20-41968573a567"
      },
      "source": [
        "!pip3 install keras-tuner -q\n",
        "!pip3 install pytrends"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pytrends\n",
            "  Downloading pytrends-4.7.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9IR4qkvxTbf"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import BayesianOptimization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers import SimpleRNN, GRU\n",
        "from keras.callbacks import CSVLogger\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from pytrends.request import TrendReq\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "import matplotlib.pyplot as plt\n",
        "# Load data from with pytrend API\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORoGdzwVoXVI"
      },
      "source": [
        "filename=[\"3_kword\",'bitcoin','covid','stock_price']\n",
        "kw_list = [\"bitcoin\",\"COVID-19\",\"stock price\"]\n",
        "btc_list=[\"bitcoin\",\"cryptocurrency\",\"blockchain+bitcoin\"]\n",
        "st_list=[\"stock+price\",\"stock+index\",\"futures+stock\"]\n",
        "cov_list=[\"Covid\",\"cough+covid\",\"symptoms+covid\"]\n",
        "kw_all=[kw_list,btc_list,st_list,cov_list]\n",
        "for i,j in zip(kw_all,filename):\n",
        "    pytrends.build_payload(i, cat=0, timeframe='today 5-y', geo='', gprop='')\n",
        "    data=pytrends.interest_over_time()\n",
        "    data=data.loc[~(data==0.0).all(axis=1)]\n",
        "    series = data.iloc[: , :-1].astype(float).sort_index()\n",
        "    series.to_csv('{}.csv'.format(j))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfuEjPsUjGdw"
      },
      "source": [
        "def to_supervised(data,dropNa = True,lag = 1):\n",
        "    df = pd.DataFrame(data)\n",
        "    column = []\n",
        "    column.append(df)\n",
        "    for i in range(1,lag+1):\n",
        "        column.append(df.shift(-i))\n",
        "    df = pd.concat(column,axis=1)\n",
        "    df.dropna(inplace = True)\n",
        "    features = data.shape[1]\n",
        "    df = df.values\n",
        "    supervised_data = df[:,:features*lag]\n",
        "    supervised_data = np.column_stack( [supervised_data, df[:,features*lag]])\n",
        "    return supervised_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpAMDmskkdSy"
      },
      "source": [
        "sys.stdout = open(\"output.txt\", \"w\")\n",
        "look_back=(12,24)\n",
        "\n",
        "\n",
        "\n",
        "  #building LSTM model \n",
        "for i in filename:\n",
        "  df = pd.read_csv(str(i)+'.csv', sep=',')\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "  df.set_index('date', inplace=True)\n",
        "  appended_df = []\n",
        "  \n",
        "  for window_size in look_back:\n",
        "    for neuron in (4,8,16,32,60):\n",
        "      values = df.values\n",
        "      scaler = MinMaxScaler()\n",
        "      scaled = scaler.fit_transform(values)\n",
        "      supervised = to_supervised(scaled,lag=window_size)\n",
        "      features = df.shape[1]\n",
        "      train_len = df.shape[0]*0.8\n",
        "      train_len = int(df.shape[0]*0.8)\n",
        "      X = supervised[:,:supervised.shape[1]-1]\n",
        "      y = supervised[:,supervised.shape[1]-1:]\n",
        "\n",
        "      x_train = X[:train_len,:]\n",
        "      x_test = X[train_len:,:]\n",
        "      y_train = y[:train_len]\n",
        "      y_test = y[train_len:]\n",
        "      x_train = x_train.reshape(x_train.shape[0], 1, window_size*features)\n",
        "      x_test = x_test.reshape(x_test.shape[0], 1, window_size*features) \n",
        "      \n",
        "\n",
        "      model = Sequential()\n",
        "      model.add(SimpleRNN(neuron, input_shape=(1, window_size*features)))\n",
        "      model.add(Dense(1))\n",
        "      print(model.summary())\n",
        "\n",
        "\n",
        "      model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "      path_checkpoint = \"model_checkpoint\"+str(i)+str(window_size)+str(neuron)+\".5h\"\n",
        "      es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "      modelckpt_callback = keras.callbacks.ModelCheckpoint( monitor=\"val_loss\",filepath=path_checkpoint,  verbose=0,  save_weights_only=True, save_best_only=True)\n",
        "      es_callback = CSVLogger(str(i)+\"_\"+str(window_size)+\"_\"+str(neuron)+'log.csv', append=True, separator=';')\n",
        "  \n",
        "      history =  model.fit( x_train,y_train, validation_data = (x_test,y_test), epochs = 100 , batch_size = 1, verbose = 2, callbacks=[es_callback, modelckpt_callback,])\n",
        "\n",
        "      #scale back the prediction to orginal scale\n",
        "      y_pred = model.predict(x_test)\n",
        "      x_test = x_test.reshape(x_test.shape[0],x_test.shape[2]*x_test.shape[1])\n",
        "\n",
        "      inv_new = np.concatenate( (y_pred, x_test[:,-2:] ) , axis =1)\n",
        "      inv_new = scaler.inverse_transform(inv_new)\n",
        "      final_pred = inv_new[:,0]\n",
        "\n",
        "      y_test = y_test.reshape( len(y_test), 1)\n",
        "      inv_new = np.concatenate( (y_test, x_test[:,-2:] ) ,axis = 1)\n",
        "      inv_new = scaler.inverse_transform(inv_new)\n",
        "      actual_pred = inv_new[:,0]    \n",
        "      error = np.sqrt(mean_squared_error(final_pred, actual_pred))\n",
        "      print('Test RMSE: %.3f' % error)    \n",
        "      print('kword: '+ str(i))      \n",
        "      print('window size: '+ str(window_size))\n",
        "      print('neuron: '+ str(window_size))\n",
        "    \n",
        "sys.stdout.close()  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}