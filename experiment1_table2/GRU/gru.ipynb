{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrwiuU4oY42PkJT5aeTs8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salta-ak/Google-trend-forecast-with-LSTM/blob/main/experiment1_table2/GRU/gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiP1rQmDw6Je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qmEKJ-N5F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef2252f-8a14-4794-8906-5f6477883abe"
      },
      "source": [
        "!pip3 install keras-tuner -q\n",
        "!pip3 install pytrends"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pytrends\n",
            "  Downloading pytrends-4.7.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9IR4qkvxTbf"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import BayesianOptimization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers import SimpleRNN, GRU\n",
        "from keras.callbacks import CSVLogger\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from pytrends.request import TrendReq\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "import matplotlib.pyplot as plt\n",
        "# Load data from with pytrend API\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORoGdzwVoXVI"
      },
      "source": [
        "filename=[\"3_kword\",'bitcoin','covid','stock_price']\n",
        "kw_list = [\"bitcoin\",\"COVID-19\",\"stock price\"]\n",
        "btc_list=[\"bitcoin\",\"cryptocurrency\",\"blockchain+bitcoin\"]\n",
        "st_list=[\"stock+price\",\"stock+index\",\"futures+stock\"]\n",
        "cov_list=[\"Covid\",\"cough+covid\",\"symptoms+covid\"]\n",
        "kw_all=[kw_list,btc_list,st_list,cov_list]\n",
        "for i,j in zip(kw_all,filename):\n",
        "    pytrends.build_payload(i, cat=0, timeframe='today 5-y', geo='', gprop='')\n",
        "    data=pytrends.interest_over_time()\n",
        "    data=data.loc[~(data==0.0).all(axis=1)]\n",
        "    series = data.iloc[: , :-1].astype(float).sort_index()\n",
        "    series.to_csv('{}.csv'.format(j))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfuEjPsUjGdw"
      },
      "source": [
        "def to_supervised(data,dropNa = True,lag = 1):\n",
        "    df = pd.DataFrame(data)\n",
        "    column = []\n",
        "    column.append(df)\n",
        "    for i in range(1,lag+1):\n",
        "        column.append(df.shift(-i))\n",
        "    df = pd.concat(column,axis=1)\n",
        "    df.dropna(inplace = True)\n",
        "    features = data.shape[1]\n",
        "    df = df.values\n",
        "    supervised_data = df[:,:features*lag]\n",
        "    supervised_data = np.column_stack( [supervised_data, df[:,features*lag]])\n",
        "    return supervised_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpAMDmskkdSy",
        "outputId": "20156745-2851-47da-8e0f-4af3f17c05a6"
      },
      "source": [
        "sys.stdout = open(\"output.txt\", \"w\")\n",
        "look_back=(12,24)\n",
        "\n",
        "\n",
        "\n",
        "  #building LSTM model \n",
        "for i in filename:\n",
        "  df = pd.read_csv(str(i)+'.csv', sep=',')\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "  df.set_index('date', inplace=True)\n",
        "  appended_df = []\n",
        "  \n",
        "  for window_size in look_back:\n",
        "    for neuron in (4,8,16,32,60):\n",
        "      values = df.values\n",
        "      scaler = MinMaxScaler()\n",
        "      scaled = scaler.fit_transform(values)\n",
        "      supervised = to_supervised(scaled,lag=window_size)\n",
        "      features = df.shape[1]\n",
        "      train_len = df.shape[0]*0.8\n",
        "      train_len = int(df.shape[0]*0.8)\n",
        "      X = supervised[:,:supervised.shape[1]-1]\n",
        "      y = supervised[:,supervised.shape[1]-1:]\n",
        "\n",
        "      x_train = X[:train_len,:]\n",
        "      x_test = X[train_len:,:]\n",
        "      y_train = y[:train_len]\n",
        "      y_test = y[train_len:]\n",
        "      x_train = x_train.reshape(x_train.shape[0], 1, window_size*features)\n",
        "      x_test = x_test.reshape(x_test.shape[0], 1, window_size*features) \n",
        "      \n",
        "\n",
        "      model = Sequential()\n",
        "      model.add(GRU(neuron, input_shape=(1, window_size*features)))\n",
        "      model.add(Dense(1))\n",
        "      print(model.summary())\n",
        "\n",
        "\n",
        "      model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "      path_checkpoint = \"model_checkpoint\"+str(i)+str(window_size)+str(neuron)+\".5h\"\n",
        "      es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "      modelckpt_callback = keras.callbacks.ModelCheckpoint( monitor=\"val_loss\",filepath=path_checkpoint,  verbose=0,  save_weights_only=True, save_best_only=True)\n",
        "      es_callback = CSVLogger(str(i)+\"_\"+str(window_size)+\"_\"+str(neuron)+'log.csv', append=True, separator=';')\n",
        "  \n",
        "      history =  model.fit( x_train,y_train, validation_data = (x_test,y_test), epochs = 100 , batch_size = 1, verbose = 2, callbacks=[es_callback, modelckpt_callback,])\n",
        "\n",
        "      #scale back the prediction to orginal scale\n",
        "      y_pred = model.predict(x_test)\n",
        "      x_test = x_test.reshape(x_test.shape[0],x_test.shape[2]*x_test.shape[1])\n",
        "\n",
        "      inv_new = np.concatenate( (y_pred, x_test[:,-2:] ) , axis =1)\n",
        "      inv_new = scaler.inverse_transform(inv_new)\n",
        "      final_pred = inv_new[:,0]\n",
        "\n",
        "      y_test = y_test.reshape( len(y_test), 1)\n",
        "      inv_new = np.concatenate( (y_test, x_test[:,-2:] ) ,axis = 1)\n",
        "      inv_new = scaler.inverse_transform(inv_new)\n",
        "      actual_pred = inv_new[:,0]    \n",
        "      error = np.sqrt(mean_squared_error(final_pred, actual_pred))\n",
        "      print('Test RMSE: %.3f' % error)    \n",
        "      print('kword: '+ str(i))      \n",
        "      print('window size: '+ str(window_size))\n",
        "      print('neuron: '+ str(window_size))\n",
        "    \n",
        "sys.stdout.close()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5425fff0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f542aa41ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}