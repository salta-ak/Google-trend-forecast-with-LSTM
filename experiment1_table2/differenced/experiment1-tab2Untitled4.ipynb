{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKG1bLH67g4o0InKTk1ELZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salta-ak/Google-trend-forecast-with-LSTM/blob/main/experiment1_table2/differenced/experiment1-tab2Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiP1rQmDw6Je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qmEKJ-N5F0"
      },
      "source": [
        "!pip3 install keras-tuner -q\n",
        "!pip3 install pytrends"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaxf8eEaxFiD"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytrends.request import TrendReq\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "# Load data from with pytrend API\n",
        "\n",
        "\n",
        "filename=['bitcoin','covid','stock_price']\n",
        "kw_list = [\"bitcoin\",\"COVID-19\",\"stock price\"]\n",
        "btc_list=[\"bitcoin\",\"cryptocurrency\",\"blockchain+bitcoin\"]\n",
        "st_list=[\"stock+price\",\"stock+index\",\"futures+stock\"]\n",
        "cov_list=[\"Covid\",\"cough+covid\",\"symptoms+covid\"]\n",
        "kw_all=[kw_list,btc_list,st_list,cov_list]\n",
        "for i,j in zip(kw_all,filename):\n",
        "    pytrends.build_payload(i, cat=0, timeframe='today 5-y', geo='', gprop='')\n",
        "    data=pytrends.interest_over_time()\n",
        "    data=data.loc[~(data==0.0).all(axis=1)]\n",
        "    series = data.iloc[: , :-1].astype(float).sort_index()\n",
        "    series.to_csv('{}.csv'.format(j))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9IR4qkvxTbf"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import BayesianOptimization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.callbacks import CSVLogger\n",
        "import sys\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('bitcoin.csv', sep=',')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True) \n",
        "\n",
        "\n",
        "def create_dataset(data_series, look_back, split_frac, transforms):\n",
        "    \n",
        "    # log transforming that data, if necessary\n",
        "    \n",
        "    # differencing data, if necessary\n",
        "    if transforms[0] == True:\n",
        "        dates = data_series.index\n",
        "        data_series = pd.Series(data_series - data_series.shift(1), index=dates).dropna()\n",
        "\n",
        "    # scaling values between 0 and 1\n",
        "    dates = data_series.index\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data_series.values.reshape(-1, 1))\n",
        "    data_series = pd.Series(scaled_data[:, 0], index=dates)\n",
        "    \n",
        "    # creating targets and features by shifting values by 'i' number of time periods\n",
        "    df = pd.DataFrame()\n",
        "    for i in range(look_back+1):\n",
        "        label = ''.join(['t-', str(i)])\n",
        "        df[label] = data_series.shift(i)\n",
        "    df = df.dropna()\n",
        "    print(df.tail())\n",
        "    \n",
        "    # splitting data into train and test sets\n",
        "    size = int(split_frac*df.shape[0])\n",
        "    train = df[:size]\n",
        "    test = df[size:]\n",
        "    \n",
        "    # creating target and features for training set\n",
        "    X_train = train.iloc[:, 1:].values\n",
        "    y_train = train.iloc[:, 0].values\n",
        "    train_dates = train.index\n",
        "    \n",
        "    # creating target and features for test set\n",
        "    X_test = test.iloc[:, 1:].values\n",
        "    y_test = test.iloc[:, 0].values\n",
        "    test_dates = test.index\n",
        "    \n",
        "    # reshaping data into 3 dimensions for modeling with the LSTM neural net\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], 1, look_back))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], 1, look_back))\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, train_dates, test_dates, scaler\n",
        "    \n",
        "    \n",
        "    \n",
        "def inverse_transforms(train_predict, y_train, test_predict, y_test, data_series, train_dates, test_dates, scaler, transforms):\n",
        "    \n",
        "    # inverse 0 to 1 scaling\n",
        "    train_predict = pd.Series(scaler.inverse_transform(train_predict.reshape(-1,1))[:,0], index=train_dates)\n",
        "    y_train = pd.Series(scaler.inverse_transform(y_train.reshape(-1, 1))[:,0], index=train_dates)\n",
        "\n",
        "    test_predict = pd.Series(scaler.inverse_transform(test_predict.reshape(-1, 1))[:,0], index=test_dates)\n",
        "    y_test = pd.Series(scaler.inverse_transform(y_test.reshape(-1, 1))[:,0], index=test_dates)\n",
        "    if transforms[0] == True:\n",
        "        train_predict = pd.Series(train_predict + data_series.shift(1), index=train_dates).dropna()\n",
        "        y_train = pd.Series(y_train + data_series.shift(1), index=train_dates).dropna()\n",
        "\n",
        "        test_predict = pd.Series(test_predict + data_series.shift(1), index=test_dates).dropna()\n",
        "        y_test = pd.Series(y_test + data_series.shift(1), index=test_dates).dropna()\n",
        "        \n",
        "    return train_predict, y_train, test_predict, y_test"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlE7PbLbxbIF"
      },
      "source": [
        "#sys.stdout = open(\"output.txt\", \"w\")\n",
        "look_back=(12,24)\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "  #building LSTM model \n",
        "for column in df: \n",
        "  for window_size in look_back:\n",
        "    for neuron in (4,8,16,32,60):\n",
        "      print(\"\\n -------------------------------- \\n Training Data \\n -------------------------------- \\n \")\n",
        "      X_train, y_train, X_test, y_test, train_dates, test_dates, scaler = create_dataset(df[column], window_size, 0.8, [True])\n",
        "      model = Sequential()\n",
        "      model.add(LSTM(neuron, input_shape=(1, window_size)))\n",
        "      model.add(Dense(1))\n",
        "      print(\"\\n -------------------------------- \\n Model Summary \\n-------------------------------- \\n \")\n",
        "      print(model.summary())\n",
        "\n",
        "      model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "      path_checkpoint = str(column)+\"_\"+str(window_size)+\"_\"+str(neuron)+\"model_checkpoint.h5\"\n",
        "      es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "      modelckpt_callback = keras.callbacks.ModelCheckpoint( monitor=\"val_loss\",filepath=path_checkpoint,  verbose=1,  save_weights_only=True, save_best_only=True)\n",
        "      es_callback = CSVLogger(str(column)+\"_\"+str(window_size)+\"_\"+str(neuron)+'log.csv', append=True, separator=';')\n",
        "\n",
        "\n",
        "      r=model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2,callbacks=[es_callback, modelckpt_callback,])\n",
        "\n",
        "# making predictions\n",
        "      train_predict = model.predict(X_train)\n",
        "      test_predict = model.predict(X_test)\n",
        "    \n",
        "# inverse transforming results\n",
        "      inverse_transforms(train_predict, y_train, test_predict, y_test, df[column], train_dates, test_dates, scaler,[True])\n",
        "      print(\"\\n -------------------------------- \\n RMSE \\n -------------------------------- \\n\")\n",
        "      error = np.sqrt(mean_squared_error(train_predict, y_train))\n",
        "      print('Train RMSE: %.3f' % error)\n",
        "      error = np.sqrt(mean_squared_error(test_predict, y_test))\n",
        "      print('Train RMSE: %.3f' % error)\n",
        "      print('key_word: ' + str(column))\n",
        "      print('window size: '+ str(window_size))\n",
        "      print('N neurons: ' + str(neuron )) \n",
        "#sys.stdout.close()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}