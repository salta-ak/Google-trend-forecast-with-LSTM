{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1AURM5usa6XjdXI8WuMdFa7xlQ_6kaqxU",
      "authorship_tag": "ABX9TyPIil6oLD0cBFoou1LFOck2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salta-ak/Google-trend-forecast-with-LSTM/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6qmEKJ-N5F0",
        "outputId": "eb26050b-78bd-4894-b5bd-9e5ccf2fdead"
      },
      "source": [
        "!pip3 install keras-tuner -q\n",
        "!pip3 install pytrends"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.0 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cS6fv7LaLea",
        "outputId": "cd59ad23-de7b-485c-93cb-cde00fac1c54"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import BayesianOptimization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('bitcoin.csv', sep=',')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True) \n",
        "\n",
        "\n",
        "def create_dataset(data_series, look_back, split_frac, transforms):\n",
        "    \n",
        "    # log transforming that data, if necessary\n",
        "    \n",
        "    # differencing data, if necessary\n",
        "    if transforms[0] == True:\n",
        "        dates = data_series.index\n",
        "        data_series = pd.Series(data_series - data_series.shift(1), index=dates).dropna()\n",
        "\n",
        "    # scaling values between 0 and 1\n",
        "    dates = data_series.index\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data_series.values.reshape(-1, 1))\n",
        "    data_series = pd.Series(scaled_data[:, 0], index=dates)\n",
        "    \n",
        "    # creating targets and features by shifting values by 'i' number of time periods\n",
        "    df = pd.DataFrame()\n",
        "    for i in range(look_back+1):\n",
        "        label = ''.join(['t-', str(i)])\n",
        "        df[label] = data_series.shift(i)\n",
        "    df = df.dropna()\n",
        "    print(df.tail())\n",
        "    \n",
        "    # splitting data into train and test sets\n",
        "    size = int(split_frac*df.shape[0])\n",
        "    train = df[:size]\n",
        "    test = df[size:]\n",
        "    \n",
        "    # creating target and features for training set\n",
        "    X_train = train.iloc[:, 1:].values\n",
        "    y_train = train.iloc[:, 0].values\n",
        "    train_dates = train.index\n",
        "    \n",
        "    # creating target and features for test set\n",
        "    X_test = test.iloc[:, 1:].values\n",
        "    y_test = test.iloc[:, 0].values\n",
        "    test_dates = test.index\n",
        "    \n",
        "    # reshaping data into 3 dimensions for modeling with the LSTM neural net\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], 1, look_back))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], 1, look_back))\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, train_dates, test_dates, scaler\n",
        "    \n",
        "    \n",
        "    \n",
        "def inverse_transforms(train_predict, y_train, test_predict, y_test, data_series, train_dates, test_dates, scaler, transforms):\n",
        "    \n",
        "    # inverse 0 to 1 scaling\n",
        "    train_predict = pd.Series(scaler.inverse_transform(train_predict.reshape(-1,1))[:,0], index=train_dates)\n",
        "    y_train = pd.Series(scaler.inverse_transform(y_train.reshape(-1, 1))[:,0], index=train_dates)\n",
        "\n",
        "    test_predict = pd.Series(scaler.inverse_transform(test_predict.reshape(-1, 1))[:,0], index=test_dates)\n",
        "    y_test = pd.Series(scaler.inverse_transform(y_test.reshape(-1, 1))[:,0], index=test_dates)\n",
        "    if transforms[0] == True:\n",
        "        train_predict = pd.Series(train_predict + data_series.shift(1), index=train_dates).dropna()\n",
        "        y_train = pd.Series(y_train + data_series.shift(1), index=train_dates).dropna()\n",
        "\n",
        "        test_predict = pd.Series(test_predict + data_series.shift(1), index=test_dates).dropna()\n",
        "        y_test = pd.Series(y_test + data_series.shift(1), index=test_dates).dropna()\n",
        "        \n",
        "    return train_predict, y_train, test_predict, y_test\n",
        "    \n",
        "    \n",
        "    #preparing our training and test dataset\n",
        "look_back = 12\n",
        "print(\"\\n -------------------------------- \\n Training Data \\n -------------------------------- \\n \")\n",
        "X_train, y_train, X_test, y_test, train_dates, test_dates, scaler = create_dataset(df.bitcoin, look_back, 0.9, [True])\n",
        "\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " -------------------------------- \n",
            " Training Data \n",
            " -------------------------------- \n",
            " \n",
            "                 t-0       t-1       t-2  ...      t-10      t-11      t-12\n",
            "date                                      ...                              \n",
            "2021-07-11  0.472973  0.405405  0.310811  ...  0.378378  0.283784  0.648649\n",
            "2021-07-18  0.513514  0.472973  0.405405  ...  0.662162  0.378378  0.283784\n",
            "2021-07-25  0.554054  0.513514  0.472973  ...  0.905405  0.662162  0.378378\n",
            "2021-08-01  0.378378  0.554054  0.513514  ...  0.175676  0.905405  0.662162\n",
            "2021-08-08  0.527027  0.378378  0.554054  ...  0.243243  0.175676  0.905405\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZtxE7xvl14n"
      },
      "source": [
        "from keras_tuner import HyperModel\n",
        "\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "\n",
        "    def build(self,hp):\n",
        "      model = keras.Sequential()\n",
        "      for i in range(hp.Int(\"num_layers\", 1, 4)):\n",
        "        model.add(LSTM(units=hp.Int(\"units_\" + str(i), min_value=8, max_value=60, step=8),return_sequences=True ))\n",
        "        #model.add(Dense(units=hp.Int(\"units_\" + str(i), min_value=8, max_value=60, step=8)))\n",
        "        model.add(Dense(1))\n",
        "        model.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3])),loss=\"mse\",metrics=[\"mse\"],)\n",
        "      return model\n",
        " \n",
        "\n",
        "hypermodel = MyHyperModel(classes=10)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTYfbqwhnCPY",
        "outputId": "152ec86c-fb22-4d88-dd25-91b9d8485176"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective=\"mse\",\n",
        "    max_trials=1,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"experement1_diff\",\n",
        "    project_name=\"RandomSearch_diff\",\n",
        ")    \n",
        "\n",
        "# search for the best hyperparameter configuration\n",
        "tuner.search(X_train, y_train, epochs=1, validation_data=(X_test, y_test))\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "print(best_hps.values)\n",
        "model=tuner.hypermodel.build(best_hps)\n",
        "model1 = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1, verbose=2)\n",
        "tuner.results_summary()\n",
        "\n",
        "\n",
        "# making predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "    \n",
        "# inverse transforming results\n",
        "train_predict, y_train, test_predict, y_test = \\\n",
        "inverse_transforms(train_predict, y_train, test_predict, y_test, df.bitcoin, train_dates, test_dates, scaler, [True])\n",
        "    \n",
        "print(\"\\n -------------------------------- \\n RMSE \\n -------------------------------- \\n\")\n",
        "error = np.sqrt(mean_squared_error(train_predict, y_train))\n",
        "print('Train RMSE: %.3f' % error)\n",
        "error = np.sqrt(mean_squared_error(test_predict, y_test))\n",
        "print('Test RMSE: %.3f' % error)\n",
        "\n",
        "path_checkpoint = \"model_checkpoint.h5\"\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "\n",
        "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    filepath=path_checkpoint,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        ")\n",
        "es_callback = CSVLogger('log.csv', append=True, separator=';')\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1, verbose=2, callbacks=[es_callback, modelckpt_callback,])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 00m 28s]\n",
            "mse: 0.22932500392198563\n",
            "\n",
            "Best mse So Far: 0.22932500392198563\n",
            "Total elapsed time: 00h 00m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "{'num_layers': 4, 'units_0': 16, 'learning_rate': 0.001, 'units_1': 8, 'units_2': 8, 'units_3': 8}\n",
            "Epoch 1/100\n",
            "222/222 - 11s - loss: 0.0650 - mse: 0.0650 - val_loss: 0.0249 - val_mse: 0.0249\n",
            "Epoch 2/100\n",
            "222/222 - 1s - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0252 - val_mse: 0.0252\n",
            "Epoch 3/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0253 - val_mse: 0.0253\n",
            "Epoch 4/100\n",
            "222/222 - 1s - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0261 - val_mse: 0.0261\n",
            "Epoch 5/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0249 - val_mse: 0.0249\n",
            "Epoch 6/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 7/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0253 - val_mse: 0.0253\n",
            "Epoch 8/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0261 - val_mse: 0.0261\n",
            "Epoch 9/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0250 - val_mse: 0.0250\n",
            "Epoch 10/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0248 - val_mse: 0.0248\n",
            "Epoch 11/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0253 - val_mse: 0.0253\n",
            "Epoch 12/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0249 - val_mse: 0.0249\n",
            "Epoch 13/100\n",
            "222/222 - 1s - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 14/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0262 - val_mse: 0.0262\n",
            "Epoch 15/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 16/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0264 - val_mse: 0.0264\n",
            "Epoch 17/100\n",
            "222/222 - 1s - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0260 - val_mse: 0.0260\n",
            "Epoch 18/100\n",
            "222/222 - 1s - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 19/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0247 - val_mse: 0.0247\n",
            "Epoch 20/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 21/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0251 - val_mse: 0.0251\n",
            "Epoch 22/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0248 - val_mse: 0.0248\n",
            "Epoch 23/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0248 - val_mse: 0.0248\n",
            "Epoch 24/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0251 - val_mse: 0.0251\n",
            "Epoch 25/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0251 - val_mse: 0.0251\n",
            "Epoch 26/100\n",
            "222/222 - 1s - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0251 - val_mse: 0.0251\n",
            "Epoch 27/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 28/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 29/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 30/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0266 - val_mse: 0.0266\n",
            "Epoch 31/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0258 - val_mse: 0.0258\n",
            "Epoch 32/100\n",
            "222/222 - 1s - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 33/100\n",
            "222/222 - 1s - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 34/100\n",
            "222/222 - 1s - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0254 - val_mse: 0.0254\n",
            "Epoch 35/100\n",
            "222/222 - 1s - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0253 - val_mse: 0.0253\n",
            "Epoch 36/100\n",
            "222/222 - 1s - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0255 - val_mse: 0.0255\n",
            "Epoch 37/100\n",
            "222/222 - 1s - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0261 - val_mse: 0.0261\n",
            "Epoch 38/100\n",
            "222/222 - 1s - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0256 - val_mse: 0.0256\n",
            "Epoch 39/100\n",
            "222/222 - 1s - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0260 - val_mse: 0.0260\n",
            "Epoch 40/100\n",
            "222/222 - 1s - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0273 - val_mse: 0.0273\n",
            "Epoch 41/100\n",
            "222/222 - 1s - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0274 - val_mse: 0.0274\n",
            "Epoch 42/100\n",
            "222/222 - 1s - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0266 - val_mse: 0.0266\n",
            "Epoch 43/100\n",
            "222/222 - 1s - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0268 - val_mse: 0.0268\n",
            "Epoch 44/100\n",
            "222/222 - 1s - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0271 - val_mse: 0.0271\n",
            "Epoch 45/100\n",
            "222/222 - 1s - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0285 - val_mse: 0.0285\n",
            "Epoch 46/100\n",
            "222/222 - 1s - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0281 - val_mse: 0.0281\n",
            "Epoch 47/100\n",
            "222/222 - 1s - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0282 - val_mse: 0.0282\n",
            "Epoch 48/100\n",
            "222/222 - 1s - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0276 - val_mse: 0.0276\n",
            "Epoch 49/100\n",
            "222/222 - 1s - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0280 - val_mse: 0.0280\n",
            "Epoch 50/100\n",
            "222/222 - 1s - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0290 - val_mse: 0.0290\n",
            "Epoch 51/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0294 - val_mse: 0.0294\n",
            "Epoch 52/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0290 - val_mse: 0.0290\n",
            "Epoch 53/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0302 - val_mse: 0.0302\n",
            "Epoch 54/100\n",
            "222/222 - 1s - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0292 - val_mse: 0.0292\n",
            "Epoch 55/100\n",
            "222/222 - 1s - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0285 - val_mse: 0.0285\n",
            "Epoch 56/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0285 - val_mse: 0.0285\n",
            "Epoch 57/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0298 - val_mse: 0.0298\n",
            "Epoch 58/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0334 - val_mse: 0.0334\n",
            "Epoch 59/100\n",
            "222/222 - 1s - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0305 - val_mse: 0.0305\n",
            "Epoch 60/100\n",
            "222/222 - 1s - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0322 - val_mse: 0.0322\n",
            "Epoch 61/100\n",
            "222/222 - 1s - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0297 - val_mse: 0.0297\n",
            "Epoch 62/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0313 - val_mse: 0.0313\n",
            "Epoch 63/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0295 - val_mse: 0.0295\n",
            "Epoch 64/100\n",
            "222/222 - 1s - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0297 - val_mse: 0.0297\n",
            "Epoch 65/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0319 - val_mse: 0.0319\n",
            "Epoch 66/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0295 - val_mse: 0.0295\n",
            "Epoch 67/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0319 - val_mse: 0.0319\n",
            "Epoch 68/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0305 - val_mse: 0.0305\n",
            "Epoch 69/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0304 - val_mse: 0.0304\n",
            "Epoch 70/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 71/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0302 - val_mse: 0.0302\n",
            "Epoch 72/100\n",
            "222/222 - 1s - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0303 - val_mse: 0.0303\n",
            "Epoch 73/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 74/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0296 - val_mse: 0.0296\n",
            "Epoch 75/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0289 - val_mse: 0.0289\n",
            "Epoch 76/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0295 - val_mse: 0.0295\n",
            "Epoch 77/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0296 - val_mse: 0.0296\n",
            "Epoch 78/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0282 - val_mse: 0.0282\n",
            "Epoch 79/100\n",
            "222/222 - 1s - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0309 - val_mse: 0.0309\n",
            "Epoch 80/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0295 - val_mse: 0.0295\n",
            "Epoch 81/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0318 - val_mse: 0.0318\n",
            "Epoch 82/100\n",
            "222/222 - 1s - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0290 - val_mse: 0.0290\n",
            "Epoch 83/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0326 - val_mse: 0.0326\n",
            "Epoch 84/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0317 - val_mse: 0.0317\n",
            "Epoch 85/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0313 - val_mse: 0.0313\n",
            "Epoch 86/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0310 - val_mse: 0.0310\n",
            "Epoch 87/100\n",
            "222/222 - 1s - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0289 - val_mse: 0.0289\n",
            "Epoch 88/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0289 - val_mse: 0.0289\n",
            "Epoch 89/100\n",
            "222/222 - 1s - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0324 - val_mse: 0.0324\n",
            "Epoch 90/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0317 - val_mse: 0.0317\n",
            "Epoch 91/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0302 - val_mse: 0.0302\n",
            "Epoch 92/100\n",
            "222/222 - 1s - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0298 - val_mse: 0.0298\n",
            "Epoch 93/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0297 - val_mse: 0.0297\n",
            "Epoch 94/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 95/100\n",
            "222/222 - 1s - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0330 - val_mse: 0.0330\n",
            "Epoch 96/100\n",
            "222/222 - 1s - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0319 - val_mse: 0.0319\n",
            "Epoch 97/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0317 - val_mse: 0.0317\n",
            "Epoch 98/100\n",
            "222/222 - 1s - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0314 - val_mse: 0.0314\n",
            "Epoch 99/100\n",
            "222/222 - 1s - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0307 - val_mse: 0.0307\n",
            "Epoch 100/100\n",
            "222/222 - 1s - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0305 - val_mse: 0.0305\n",
            "Results summary\n",
            "Results in experement1_diff/RandomSearch_diff\n",
            "Showing 10 best trials\n",
            "Objective(name='mse', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "units_0: 16\n",
            "learning_rate: 0.001\n",
            "units_1: 8\n",
            "units_2: 8\n",
            "units_3: 8\n",
            "Score: 0.22932500392198563\n",
            "\n",
            " -------------------------------- \n",
            " RMSE \n",
            " -------------------------------- \n",
            "\n",
            "Train RMSE: 5.405\n",
            "Test RMSE: 12.933\n",
            "Epoch 1/100\n",
            "222/222 - 6s - loss: 372.2779 - mse: 372.2779 - val_loss: 1303.1621 - val_mse: 1303.1621\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1303.16211, saving model to model_checkpoint.h5\n",
            "Epoch 2/100\n",
            "222/222 - 1s - loss: 295.8578 - mse: 295.8578 - val_loss: 1183.6696 - val_mse: 1183.6696\n",
            "\n",
            "Epoch 00002: val_loss improved from 1303.16211 to 1183.66956, saving model to model_checkpoint.h5\n",
            "Epoch 3/100\n",
            "222/222 - 1s - loss: 275.5136 - mse: 275.5136 - val_loss: 1116.6705 - val_mse: 1116.6705\n",
            "\n",
            "Epoch 00003: val_loss improved from 1183.66956 to 1116.67053, saving model to model_checkpoint.h5\n",
            "Epoch 4/100\n",
            "222/222 - 1s - loss: 264.0735 - mse: 264.0735 - val_loss: 1070.0654 - val_mse: 1070.0654\n",
            "\n",
            "Epoch 00004: val_loss improved from 1116.67053 to 1070.06543, saving model to model_checkpoint.h5\n",
            "Epoch 5/100\n",
            "222/222 - 2s - loss: 256.2481 - mse: 256.2481 - val_loss: 1032.0229 - val_mse: 1032.0229\n",
            "\n",
            "Epoch 00005: val_loss improved from 1070.06543 to 1032.02295, saving model to model_checkpoint.h5\n",
            "Epoch 6/100\n",
            "222/222 - 1s - loss: 250.6161 - mse: 250.6161 - val_loss: 1001.3723 - val_mse: 1001.3723\n",
            "\n",
            "Epoch 00006: val_loss improved from 1032.02295 to 1001.37231, saving model to model_checkpoint.h5\n",
            "Epoch 7/100\n",
            "222/222 - 1s - loss: 246.2177 - mse: 246.2177 - val_loss: 973.3090 - val_mse: 973.3090\n",
            "\n",
            "Epoch 00007: val_loss improved from 1001.37231 to 973.30896, saving model to model_checkpoint.h5\n",
            "Epoch 8/100\n",
            "222/222 - 1s - loss: 242.9534 - mse: 242.9534 - val_loss: 952.9852 - val_mse: 952.9852\n",
            "\n",
            "Epoch 00008: val_loss improved from 973.30896 to 952.98523, saving model to model_checkpoint.h5\n",
            "Epoch 9/100\n",
            "222/222 - 1s - loss: 240.3766 - mse: 240.3766 - val_loss: 932.9301 - val_mse: 932.9301\n",
            "\n",
            "Epoch 00009: val_loss improved from 952.98523 to 932.93005, saving model to model_checkpoint.h5\n",
            "Epoch 10/100\n",
            "222/222 - 1s - loss: 238.3888 - mse: 238.3888 - val_loss: 915.2847 - val_mse: 915.2847\n",
            "\n",
            "Epoch 00010: val_loss improved from 932.93005 to 915.28467, saving model to model_checkpoint.h5\n",
            "Epoch 11/100\n",
            "222/222 - 1s - loss: 236.8278 - mse: 236.8278 - val_loss: 900.8011 - val_mse: 900.8011\n",
            "\n",
            "Epoch 00011: val_loss improved from 915.28467 to 900.80109, saving model to model_checkpoint.h5\n",
            "Epoch 12/100\n",
            "222/222 - 1s - loss: 235.5129 - mse: 235.5129 - val_loss: 887.3682 - val_mse: 887.3682\n",
            "\n",
            "Epoch 00012: val_loss improved from 900.80109 to 887.36823, saving model to model_checkpoint.h5\n",
            "Epoch 13/100\n",
            "222/222 - 1s - loss: 234.4653 - mse: 234.4653 - val_loss: 876.6244 - val_mse: 876.6244\n",
            "\n",
            "Epoch 00013: val_loss improved from 887.36823 to 876.62439, saving model to model_checkpoint.h5\n",
            "Epoch 14/100\n",
            "222/222 - 1s - loss: 233.6165 - mse: 233.6165 - val_loss: 865.6509 - val_mse: 865.6509\n",
            "\n",
            "Epoch 00014: val_loss improved from 876.62439 to 865.65088, saving model to model_checkpoint.h5\n",
            "Epoch 15/100\n",
            "222/222 - 1s - loss: 232.9580 - mse: 232.9580 - val_loss: 855.4895 - val_mse: 855.4895\n",
            "\n",
            "Epoch 00015: val_loss improved from 865.65088 to 855.48950, saving model to model_checkpoint.h5\n",
            "Epoch 16/100\n",
            "222/222 - 1s - loss: 232.4097 - mse: 232.4097 - val_loss: 847.2690 - val_mse: 847.2690\n",
            "\n",
            "Epoch 00016: val_loss improved from 855.48950 to 847.26898, saving model to model_checkpoint.h5\n",
            "Epoch 17/100\n",
            "222/222 - 1s - loss: 231.9599 - mse: 231.9599 - val_loss: 840.2020 - val_mse: 840.2020\n",
            "\n",
            "Epoch 00017: val_loss improved from 847.26898 to 840.20203, saving model to model_checkpoint.h5\n",
            "Epoch 18/100\n",
            "222/222 - 1s - loss: 231.6200 - mse: 231.6200 - val_loss: 834.6052 - val_mse: 834.6052\n",
            "\n",
            "Epoch 00018: val_loss improved from 840.20203 to 834.60522, saving model to model_checkpoint.h5\n",
            "Epoch 19/100\n",
            "222/222 - 1s - loss: 231.3212 - mse: 231.3212 - val_loss: 827.5737 - val_mse: 827.5737\n",
            "\n",
            "Epoch 00019: val_loss improved from 834.60522 to 827.57373, saving model to model_checkpoint.h5\n",
            "Epoch 20/100\n",
            "222/222 - 1s - loss: 231.0801 - mse: 231.0801 - val_loss: 822.0648 - val_mse: 822.0648\n",
            "\n",
            "Epoch 00020: val_loss improved from 827.57373 to 822.06476, saving model to model_checkpoint.h5\n",
            "Epoch 21/100\n",
            "222/222 - 1s - loss: 230.9115 - mse: 230.9115 - val_loss: 817.1501 - val_mse: 817.1501\n",
            "\n",
            "Epoch 00021: val_loss improved from 822.06476 to 817.15015, saving model to model_checkpoint.h5\n",
            "Epoch 22/100\n",
            "222/222 - 1s - loss: 230.7675 - mse: 230.7675 - val_loss: 813.4359 - val_mse: 813.4359\n",
            "\n",
            "Epoch 00022: val_loss improved from 817.15015 to 813.43591, saving model to model_checkpoint.h5\n",
            "Epoch 23/100\n",
            "222/222 - 1s - loss: 230.6502 - mse: 230.6502 - val_loss: 809.6596 - val_mse: 809.6596\n",
            "\n",
            "Epoch 00023: val_loss improved from 813.43591 to 809.65961, saving model to model_checkpoint.h5\n",
            "Epoch 24/100\n",
            "222/222 - 1s - loss: 230.5194 - mse: 230.5194 - val_loss: 805.6691 - val_mse: 805.6691\n",
            "\n",
            "Epoch 00024: val_loss improved from 809.65961 to 805.66913, saving model to model_checkpoint.h5\n",
            "Epoch 25/100\n",
            "222/222 - 1s - loss: 230.4258 - mse: 230.4258 - val_loss: 802.6680 - val_mse: 802.6680\n",
            "\n",
            "Epoch 00025: val_loss improved from 805.66913 to 802.66797, saving model to model_checkpoint.h5\n",
            "Epoch 26/100\n",
            "222/222 - 1s - loss: 230.3685 - mse: 230.3685 - val_loss: 801.1782 - val_mse: 801.1782\n",
            "\n",
            "Epoch 00026: val_loss improved from 802.66797 to 801.17822, saving model to model_checkpoint.h5\n",
            "Epoch 27/100\n",
            "222/222 - 1s - loss: 230.3065 - mse: 230.3065 - val_loss: 797.2877 - val_mse: 797.2877\n",
            "\n",
            "Epoch 00027: val_loss improved from 801.17822 to 797.28772, saving model to model_checkpoint.h5\n",
            "Epoch 28/100\n",
            "222/222 - 1s - loss: 230.2579 - mse: 230.2579 - val_loss: 796.4347 - val_mse: 796.4347\n",
            "\n",
            "Epoch 00028: val_loss improved from 797.28772 to 796.43469, saving model to model_checkpoint.h5\n",
            "Epoch 29/100\n",
            "222/222 - 1s - loss: 230.1999 - mse: 230.1999 - val_loss: 792.7547 - val_mse: 792.7547\n",
            "\n",
            "Epoch 00029: val_loss improved from 796.43469 to 792.75470, saving model to model_checkpoint.h5\n",
            "Epoch 30/100\n",
            "222/222 - 1s - loss: 230.2580 - mse: 230.2580 - val_loss: 791.9952 - val_mse: 791.9952\n",
            "\n",
            "Epoch 00030: val_loss improved from 792.75470 to 791.99524, saving model to model_checkpoint.h5\n",
            "Epoch 31/100\n",
            "222/222 - 1s - loss: 230.2116 - mse: 230.2116 - val_loss: 789.2757 - val_mse: 789.2757\n",
            "\n",
            "Epoch 00031: val_loss improved from 791.99524 to 789.27570, saving model to model_checkpoint.h5\n",
            "Epoch 32/100\n",
            "222/222 - 1s - loss: 230.1656 - mse: 230.1656 - val_loss: 789.1671 - val_mse: 789.1671\n",
            "\n",
            "Epoch 00032: val_loss improved from 789.27570 to 789.16705, saving model to model_checkpoint.h5\n",
            "Epoch 33/100\n",
            "222/222 - 1s - loss: 230.1378 - mse: 230.1378 - val_loss: 787.5142 - val_mse: 787.5142\n",
            "\n",
            "Epoch 00033: val_loss improved from 789.16705 to 787.51422, saving model to model_checkpoint.h5\n",
            "Epoch 34/100\n",
            "222/222 - 1s - loss: 230.1319 - mse: 230.1319 - val_loss: 784.8408 - val_mse: 784.8408\n",
            "\n",
            "Epoch 00034: val_loss improved from 787.51422 to 784.84076, saving model to model_checkpoint.h5\n",
            "Epoch 35/100\n",
            "222/222 - 1s - loss: 230.1515 - mse: 230.1515 - val_loss: 784.0668 - val_mse: 784.0668\n",
            "\n",
            "Epoch 00035: val_loss improved from 784.84076 to 784.06677, saving model to model_checkpoint.h5\n",
            "Epoch 36/100\n",
            "222/222 - 1s - loss: 230.1750 - mse: 230.1750 - val_loss: 783.2009 - val_mse: 783.2009\n",
            "\n",
            "Epoch 00036: val_loss improved from 784.06677 to 783.20093, saving model to model_checkpoint.h5\n",
            "Epoch 37/100\n",
            "222/222 - 1s - loss: 230.1103 - mse: 230.1103 - val_loss: 782.6245 - val_mse: 782.6245\n",
            "\n",
            "Epoch 00037: val_loss improved from 783.20093 to 782.62445, saving model to model_checkpoint.h5\n",
            "Epoch 38/100\n",
            "222/222 - 1s - loss: 230.0589 - mse: 230.0589 - val_loss: 782.7382 - val_mse: 782.7382\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 782.62445\n",
            "Epoch 39/100\n",
            "222/222 - 1s - loss: 230.0759 - mse: 230.0759 - val_loss: 780.9402 - val_mse: 780.9402\n",
            "\n",
            "Epoch 00039: val_loss improved from 782.62445 to 780.94025, saving model to model_checkpoint.h5\n",
            "Epoch 40/100\n",
            "222/222 - 1s - loss: 230.0764 - mse: 230.0764 - val_loss: 780.5224 - val_mse: 780.5224\n",
            "\n",
            "Epoch 00040: val_loss improved from 780.94025 to 780.52240, saving model to model_checkpoint.h5\n",
            "Epoch 41/100\n",
            "222/222 - 1s - loss: 230.1097 - mse: 230.1097 - val_loss: 780.0664 - val_mse: 780.0664\n",
            "\n",
            "Epoch 00041: val_loss improved from 780.52240 to 780.06641, saving model to model_checkpoint.h5\n",
            "Epoch 42/100\n",
            "222/222 - 1s - loss: 230.1385 - mse: 230.1385 - val_loss: 779.2952 - val_mse: 779.2952\n",
            "\n",
            "Epoch 00042: val_loss improved from 780.06641 to 779.29523, saving model to model_checkpoint.h5\n",
            "Epoch 43/100\n",
            "222/222 - 1s - loss: 230.1237 - mse: 230.1237 - val_loss: 779.5980 - val_mse: 779.5980\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 779.29523\n",
            "Epoch 44/100\n",
            "222/222 - 1s - loss: 230.0854 - mse: 230.0854 - val_loss: 779.4484 - val_mse: 779.4484\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 779.29523\n",
            "Epoch 45/100\n",
            "222/222 - 1s - loss: 230.0811 - mse: 230.0811 - val_loss: 779.3523 - val_mse: 779.3523\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 779.29523\n",
            "Epoch 46/100\n",
            "222/222 - 1s - loss: 230.0556 - mse: 230.0556 - val_loss: 778.9612 - val_mse: 778.9612\n",
            "\n",
            "Epoch 00046: val_loss improved from 779.29523 to 778.96124, saving model to model_checkpoint.h5\n",
            "Epoch 47/100\n",
            "222/222 - 1s - loss: 230.0829 - mse: 230.0829 - val_loss: 778.1813 - val_mse: 778.1813\n",
            "\n",
            "Epoch 00047: val_loss improved from 778.96124 to 778.18134, saving model to model_checkpoint.h5\n",
            "Epoch 48/100\n",
            "222/222 - 1s - loss: 230.0691 - mse: 230.0691 - val_loss: 777.2078 - val_mse: 777.2078\n",
            "\n",
            "Epoch 00048: val_loss improved from 778.18134 to 777.20782, saving model to model_checkpoint.h5\n",
            "Epoch 49/100\n",
            "222/222 - 1s - loss: 230.0576 - mse: 230.0576 - val_loss: 776.6577 - val_mse: 776.6577\n",
            "\n",
            "Epoch 00049: val_loss improved from 777.20782 to 776.65765, saving model to model_checkpoint.h5\n",
            "Epoch 50/100\n",
            "222/222 - 1s - loss: 230.0807 - mse: 230.0807 - val_loss: 776.8262 - val_mse: 776.8262\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 776.65765\n",
            "Epoch 51/100\n",
            "222/222 - 1s - loss: 230.0860 - mse: 230.0860 - val_loss: 777.3895 - val_mse: 777.3895\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 776.65765\n",
            "Epoch 52/100\n",
            "222/222 - 1s - loss: 230.1000 - mse: 230.1000 - val_loss: 776.3688 - val_mse: 776.3688\n",
            "\n",
            "Epoch 00052: val_loss improved from 776.65765 to 776.36884, saving model to model_checkpoint.h5\n",
            "Epoch 53/100\n",
            "222/222 - 1s - loss: 230.0432 - mse: 230.0432 - val_loss: 776.1384 - val_mse: 776.1384\n",
            "\n",
            "Epoch 00053: val_loss improved from 776.36884 to 776.13843, saving model to model_checkpoint.h5\n",
            "Epoch 54/100\n",
            "222/222 - 1s - loss: 230.0624 - mse: 230.0624 - val_loss: 777.0187 - val_mse: 777.0187\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 776.13843\n",
            "Epoch 55/100\n",
            "222/222 - 1s - loss: 230.0657 - mse: 230.0657 - val_loss: 775.4917 - val_mse: 775.4917\n",
            "\n",
            "Epoch 00055: val_loss improved from 776.13843 to 775.49170, saving model to model_checkpoint.h5\n",
            "Epoch 56/100\n",
            "222/222 - 1s - loss: 230.0383 - mse: 230.0383 - val_loss: 775.9014 - val_mse: 775.9014\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 775.49170\n",
            "Epoch 57/100\n",
            "222/222 - 1s - loss: 230.0748 - mse: 230.0748 - val_loss: 775.7079 - val_mse: 775.7079\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 775.49170\n",
            "Epoch 58/100\n",
            "222/222 - 1s - loss: 230.0823 - mse: 230.0823 - val_loss: 775.1718 - val_mse: 775.1718\n",
            "\n",
            "Epoch 00058: val_loss improved from 775.49170 to 775.17181, saving model to model_checkpoint.h5\n",
            "Epoch 59/100\n",
            "222/222 - 1s - loss: 230.1145 - mse: 230.1145 - val_loss: 775.2160 - val_mse: 775.2160\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 775.17181\n",
            "Epoch 60/100\n",
            "222/222 - 1s - loss: 230.1320 - mse: 230.1320 - val_loss: 775.9625 - val_mse: 775.9625\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 775.17181\n",
            "Epoch 61/100\n",
            "222/222 - 1s - loss: 230.0661 - mse: 230.0661 - val_loss: 775.6520 - val_mse: 775.6520\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 775.17181\n",
            "Epoch 62/100\n",
            "222/222 - 1s - loss: 230.1106 - mse: 230.1106 - val_loss: 776.0779 - val_mse: 776.0779\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 775.17181\n",
            "Epoch 63/100\n",
            "222/222 - 1s - loss: 230.0926 - mse: 230.0926 - val_loss: 775.2315 - val_mse: 775.2315\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 775.17181\n",
            "Epoch 64/100\n",
            "222/222 - 1s - loss: 230.0441 - mse: 230.0441 - val_loss: 774.4677 - val_mse: 774.4677\n",
            "\n",
            "Epoch 00064: val_loss improved from 775.17181 to 774.46765, saving model to model_checkpoint.h5\n",
            "Epoch 65/100\n",
            "222/222 - 1s - loss: 230.0576 - mse: 230.0576 - val_loss: 774.3356 - val_mse: 774.3356\n",
            "\n",
            "Epoch 00065: val_loss improved from 774.46765 to 774.33563, saving model to model_checkpoint.h5\n",
            "Epoch 66/100\n",
            "222/222 - 1s - loss: 230.1061 - mse: 230.1061 - val_loss: 774.0536 - val_mse: 774.0536\n",
            "\n",
            "Epoch 00066: val_loss improved from 774.33563 to 774.05365, saving model to model_checkpoint.h5\n",
            "Epoch 67/100\n",
            "222/222 - 1s - loss: 230.0845 - mse: 230.0845 - val_loss: 776.3500 - val_mse: 776.3500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 774.05365\n",
            "Epoch 68/100\n",
            "222/222 - 1s - loss: 230.0731 - mse: 230.0731 - val_loss: 774.7916 - val_mse: 774.7916\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 774.05365\n",
            "Epoch 69/100\n",
            "222/222 - 1s - loss: 230.0739 - mse: 230.0739 - val_loss: 774.3881 - val_mse: 774.3881\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 774.05365\n",
            "Epoch 70/100\n",
            "222/222 - 1s - loss: 230.0425 - mse: 230.0425 - val_loss: 774.5483 - val_mse: 774.5483\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 774.05365\n",
            "Epoch 71/100\n",
            "222/222 - 1s - loss: 230.0669 - mse: 230.0669 - val_loss: 775.8435 - val_mse: 775.8435\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 774.05365\n",
            "Epoch 72/100\n",
            "222/222 - 1s - loss: 230.0863 - mse: 230.0863 - val_loss: 774.6370 - val_mse: 774.6370\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 774.05365\n",
            "Epoch 73/100\n",
            "222/222 - 1s - loss: 230.0805 - mse: 230.0805 - val_loss: 774.8140 - val_mse: 774.8140\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 774.05365\n",
            "Epoch 74/100\n",
            "222/222 - 1s - loss: 230.0694 - mse: 230.0694 - val_loss: 773.4288 - val_mse: 773.4288\n",
            "\n",
            "Epoch 00074: val_loss improved from 774.05365 to 773.42883, saving model to model_checkpoint.h5\n",
            "Epoch 75/100\n",
            "222/222 - 1s - loss: 230.0810 - mse: 230.0810 - val_loss: 773.7816 - val_mse: 773.7816\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 773.42883\n",
            "Epoch 76/100\n",
            "222/222 - 1s - loss: 230.0478 - mse: 230.0478 - val_loss: 774.0754 - val_mse: 774.0754\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 773.42883\n",
            "Epoch 77/100\n",
            "222/222 - 1s - loss: 230.0865 - mse: 230.0865 - val_loss: 774.7162 - val_mse: 774.7162\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 773.42883\n",
            "Epoch 78/100\n",
            "222/222 - 1s - loss: 230.0976 - mse: 230.0976 - val_loss: 774.5988 - val_mse: 774.5988\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 773.42883\n",
            "Epoch 79/100\n",
            "222/222 - 1s - loss: 230.0589 - mse: 230.0589 - val_loss: 774.3847 - val_mse: 774.3847\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 773.42883\n",
            "Epoch 80/100\n",
            "222/222 - 1s - loss: 230.0942 - mse: 230.0942 - val_loss: 774.2500 - val_mse: 774.2500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 773.42883\n",
            "Epoch 81/100\n",
            "222/222 - 1s - loss: 230.1164 - mse: 230.1164 - val_loss: 774.2964 - val_mse: 774.2964\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 773.42883\n",
            "Epoch 82/100\n",
            "222/222 - 1s - loss: 230.0673 - mse: 230.0673 - val_loss: 775.4457 - val_mse: 775.4457\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 773.42883\n",
            "Epoch 83/100\n",
            "222/222 - 1s - loss: 230.0937 - mse: 230.0937 - val_loss: 774.5734 - val_mse: 774.5734\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 773.42883\n",
            "Epoch 84/100\n",
            "222/222 - 1s - loss: 230.0816 - mse: 230.0816 - val_loss: 774.8644 - val_mse: 774.8644\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 773.42883\n",
            "Epoch 85/100\n",
            "222/222 - 1s - loss: 230.0888 - mse: 230.0888 - val_loss: 774.5012 - val_mse: 774.5012\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 773.42883\n",
            "Epoch 86/100\n",
            "222/222 - 1s - loss: 230.0416 - mse: 230.0416 - val_loss: 776.0934 - val_mse: 776.0934\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 773.42883\n",
            "Epoch 87/100\n",
            "222/222 - 1s - loss: 230.0848 - mse: 230.0848 - val_loss: 774.6508 - val_mse: 774.6508\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 773.42883\n",
            "Epoch 88/100\n",
            "222/222 - 1s - loss: 230.0531 - mse: 230.0531 - val_loss: 774.1378 - val_mse: 774.1378\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 773.42883\n",
            "Epoch 89/100\n",
            "222/222 - 1s - loss: 230.0940 - mse: 230.0940 - val_loss: 774.6423 - val_mse: 774.6423\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 773.42883\n",
            "Epoch 90/100\n",
            "222/222 - 1s - loss: 230.0788 - mse: 230.0788 - val_loss: 774.5036 - val_mse: 774.5036\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 773.42883\n",
            "Epoch 91/100\n",
            "222/222 - 1s - loss: 230.0493 - mse: 230.0493 - val_loss: 774.7737 - val_mse: 774.7737\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 773.42883\n",
            "Epoch 92/100\n",
            "222/222 - 1s - loss: 230.0403 - mse: 230.0403 - val_loss: 774.9716 - val_mse: 774.9716\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 773.42883\n",
            "Epoch 93/100\n",
            "222/222 - 1s - loss: 230.1066 - mse: 230.1066 - val_loss: 774.8791 - val_mse: 774.8791\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 773.42883\n",
            "Epoch 94/100\n",
            "222/222 - 1s - loss: 230.1026 - mse: 230.1026 - val_loss: 775.2073 - val_mse: 775.2073\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 773.42883\n",
            "Epoch 95/100\n",
            "222/222 - 1s - loss: 230.0490 - mse: 230.0490 - val_loss: 774.9643 - val_mse: 774.9643\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 773.42883\n",
            "Epoch 96/100\n",
            "222/222 - 1s - loss: 230.0640 - mse: 230.0640 - val_loss: 775.6556 - val_mse: 775.6556\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 773.42883\n",
            "Epoch 97/100\n",
            "222/222 - 1s - loss: 230.0282 - mse: 230.0282 - val_loss: 775.0522 - val_mse: 775.0522\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 773.42883\n",
            "Epoch 98/100\n",
            "222/222 - 1s - loss: 230.0690 - mse: 230.0690 - val_loss: 775.2950 - val_mse: 775.2950\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 773.42883\n",
            "Epoch 99/100\n",
            "222/222 - 1s - loss: 230.0657 - mse: 230.0657 - val_loss: 775.2126 - val_mse: 775.2126\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 773.42883\n",
            "Epoch 100/100\n",
            "222/222 - 1s - loss: 230.0659 - mse: 230.0659 - val_loss: 775.0692 - val_mse: 775.0692\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 773.42883\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}